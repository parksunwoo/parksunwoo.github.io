---
title: NAVER Deview 2023 Day2 Review
categories:
  - Dev
tags:
  - deview 2023
  - webtoon
  - nsml
last_modified_at: 2023-03-05T23:50:00-00:00
---

Day2 세션들 중 관심있는 세션들을 슬라이드 내용을 중심으로 요약정리했습니다

---
# [WebtoonMe 개발기: 내가 웹툰 안으로 들어가면 여신강림 임주경 보다 이쁠까 안이쁠까?](https://deview.kr/2023/sessions/565)

김승권, 백지혜/ **NAVER WEBTOON**

### 1. WebtoonMe 소개: 내가 웹툰으로 들어가면 어떤 느낌일까?

### 2. "웹툰으로 변환" 여정을 위한 모델 개발

- 생성 모델의 트렌드를 읊고 이를 바로 적용하자
    - 기존에는 성능이 좋은 “모델”을 만드는 것이 중요했다면,
    - 최근에는 이 모델을 활용하여 task에 적합한 (1) 고품질의 “데이터”를 생성한 후
    - (2) 가볍고 학습이 쉬운 별도의 network를 학습시키는 것이 트렌드
    - 웹툰 캐릭터의 경우 데포로메가 심하기 때문에 얼굴 변환이 잘 안되는 문제 발생
- 현업의 니즈를 바로바로 적용하자
    - 다양한 피부 톤 반영
    - 다양한 조명 조건 반영
    - 캐릭터 고유 피부 반영
    - 추론 속도 개선
    - 빠른 Ad-hoc 기능 반영

### 3. 힘들게 만든 모델, 어떻게 밖으로 내보내지?

- 빠른 프로덕트 프로토타이핑
    - 추상화 높은 방법론으로 최대한 빠른 결과로 작지만 유의미한 성공을
    - 보여주고
    - 이를 바탕으로 전문가를 섭외해서 추상화를 낮추자!
- 만들었는데 어떻게 홍보를 해야하지?

### 4. WebtoonMe 이렇게 활용 했어요.

- 방송에 데뷔한 네이버 쇼핑라이브 기술 지원 사례
- 직접 웹툰 컷으로 들어가 볼 수 있는 실시간 체험 존 구축
- 목마른 사람이 우물파는 컨셉의 현업 기술 지원

### 5. WebtoonMe 다음은?

- WebtoonMe 판을 키워보기
- 높은 개인화, 실시간 변환, 다양한 웹툰 캐릭터 지원

# [NSML: 대규모 HPC 클러스터의 효율적 활용을 위한 Scheduler, Monitoring, Diagnostics](https://deview.kr/2023/sessions/550)

박흥석,조애리,이하영/ **NAVER Cloud**

### 1. Introduction

- NAVER 대규모 분산학습 플랫폼: NSML
    - 고성능 하드웨어와 고용량의 스토리지를 쉽게 사용할 수 있도록 학습 모델링 관련 도구를 제공하는 플랫폼
    - NSML Pods는 최대 GPU 128장 규모의 모델까지 학습 가능한 대규모 HPC 클러스터
    - NSML Pod 내 모든 GPU 서버는 고속 네트워크 InfiniBand(IB)로 연결
    - NSML Pod 내 GPU 서버 간 통신 병목 최소화 → 분산 학습에 유리
- NSML의 대규모 HPC 클러스터와 효율화 전략
    - Scheduling (GPU 활용률을 높이는 스케쥴링)
    - Monitoring (대규모 HPC 클러스터 모니터링 도구 개발)
    - Diagnostics (정형화하기 어려운 분산학습 엔지니어링 이슈 진단)
    - 

### 2. Scheduler

- GPU 점유 환경의 문제점
    - GPU 서버를 팀별로 구입하고 점유해 사용
    - 흩어진 GPU 자원을 모아 효율적으로 배분하기 위해 수요 및 정책에 따른 스케쥴링 도입
- GPU 활용률을 높이기 효율적인 스케줄링 전략
    - 자원 파편화 현상 : 할당량 감소로 이어짐
    - 파편화 (Fragmentation)
        - 자원을 배정하고 회수하는 과정에서 발생하는 작은 리소스 조각
        - 파편화된 리소스들은 사용되지 못하고 방치됨
    - 할당률을 저해하는 가장 큰 요소
        - 사용자의 실험 스펙상이
        - 호스트에 종속되어 있는 자원
    - 리소스 유형 정의
        - GPU 개수에 비례하여 CPU, RAM 등의 호스트 자원을 배정
        - 호스트 내 자원 파편화 방지
    - MostAllocated 정책 적용
        - 소규모 실험이 대규모 실험의 할당을 저해해서는 안 됨
        - 리소스를 많이 사용 중인 호스트부터 우선 배치
    - 실험 규모에 따라 NSML Pod에 역할 부여
        - 규모가 유사한 실험끼리 조합해서 배치
        - 다양한 실험 규모에 따른 호스트 자원 파편화 방지
    - 할당량 향상으로는 한계가 있다
        - 수요에 비해 상대적으로 부족한 공급량
- NSML 스케줄러 도입기
    - 경제학 전문가와 협업하여 자원 활용에 대한 정성적 / 정량적 분석 진행
    - GPU 활용률
    - 리소스 유형별 대기 시간
    - 팀별 리소스 사용 현황
    - 규모에 따른 구역 활용률
        - 전체 사용자별 GPU 점유 시간
        - 우선순위 낮은 소규모 실험 활용률
    - 커스텀 스케쥴링 도입
        - k8s Scheduler Framework를 활용해 커스터마이징
        - GPU 활용률을 높이는 방향으로 할당 제어
    - 정책 우선순위 기반 스코어링
        - 실험 규모, 대기 시간에 따른 스케쥴링 우선순위 조정
    - 중, 대규모 학습 예약
        - 규모가 큰 실험에 대한 예약 시스템 도입
    - 분산학습 지원
        - 반드시 같은 NSML Pod 내에 배치
        - 동시 실행 보장 (coscheduling)

### 3. Monitoring

- 대규모 HPC 클러스터 관측의 배경
    - 자원 활용량의 극대화: 운영자 & 사용자 모두의 노력 필요
- 기존 모니터링 & MLOps 도구의 한계
    - MLOps 플랫폼: 모델 학습에 대한 가시성 제공에 집중, 개별 학습에 대한 단순 시스템 지표 제공
    - Cloud 모니터링 플랫폼: 서비스 인프라 모니터링에 집중, 학습 이상 상태 탐지 기능 구축 어려움
    - 클러스터 자원 현황과 학습 상태를 한눈에 파악할 수 있는 “통합된 가시성” 제공
- 모니터링 도구 개발 과정
    - 전체 클러스터 자원(수천 개의 GPU) 상태와 할당된 실험 상태를 동시에 표현해야 함
        - Unit visualization
        - 사용자의 멘탈 모델과 가장 일치하는 방식, GPU 자원 하나를 정사각형 유닛으로 매칭하여 시각화
        - 유닛에 생삭, 패턴을 매핑하여 아웃라이어를 빠르게 드러낼 수 있음
        - 웹 어플리케이션
            - deck.gl(WebGL 프레임워크)로 구현
            - 유닛 위치/컬러 트랜지션을 통해 클러스터 자원의 실시간 상태 변화를 직관적으로 이해
    - 운영자/사용자의 서로 다른 모니터링 니즈를 만족해야 함
        - Atomic Unit Visualization Grammar 논문의 유닛 시각화 활용 기법 구현
        - 클러스터의 물리적 요소, 실험의 논리적 요소를 포함한 다양한 데이터 속성을 계층 구조로 표현할수 있도록 설계
    - 자원이나 실험의 이상 현상을 빠르게 탐지할 수 있도록 도와야 함
        - Violation rule을 통한 힌트 제공
            - 학습 시작 시점부터 GPU 활용률 등 시스템 지표의 95%, 25% 분위값을 춫적
            - 사용자가 정의한 임계치 이하의 실험들을 하이라이팅
            - 강조된 실험들은 빠르게 진단 도구로 분석하도록 유도
    - 문제를 디버깅할 수 있는 상세한 시슽메 지표 데이터를 제공해야 함
- HPC 클러스터 관측하기

### 4. Diagnostics

- 분산학습 시 마주하는 엔지니어링 이슈
    - Data Parallelism: 데이터를 머신별로 나누어 학습
    - Model Parallelism: 모델을 머신별로 나누어 학습
    - 동기화 병목: 일부 Worker의 작업이 지연될 수 있음
        - workload imbalance 로 인한 작업 지연
        - 일부 노드 / GPU의 장애로 연산 지연
        - 노드 간 / GPU 간 데이터 통신 지연
    - 강력하지만, 대규모 실험의 엔지니어링 이슈를 효과적으로 드러내지 못함
    - 분산학습 전략에 따른 다양한 지표 패턴
- 분산학습 진단 도구 개발 과정
- 분산학습 엔지니어링 이슈 진단하기
    - 어떤 GPU/ Node를 분석해야 하는가?
        - 각 GPU 들의 지표 분포 / 패턴이 드러나도록 지표 preview 시각화
        - Violation rule에 해당하는 GPU는 강조
        - + Clustering 을 통해 확연한 차이 및 잠재된 imbalane / outlier 표현
    - 어떤 구간을 분석해야 하는가?
        - 시스템 지표 간 상관관계 분석
        - 상관관계 및 분포에서 멀리 떨어진 이상치 데이터 강조
        - 이상치 데이터와 상호작용, 해당 구간 상세 분석
    - 이상치를 직관적으로 드러내고, 효율적인 분석 방향 제시
        
        
    
    ### 5. What’s Next?
    
    - Scheduling
        - 데이터 기반 우선순위 스케쥴링 강화 (사용자의 월 소비량, GPU 활용률 실적 등)
    - Monitoring
        - 커스텀 가능한 학습 이상 상태 탐지 조건
    - Diagnostics
        - 이상치 탐지 방법론 고도화 (root cause 가시화)
        - 분산 학습 엔지니어링 이슈 알람 체계 구축