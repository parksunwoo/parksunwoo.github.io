---
title: GPTë¥¼ í™œìš©í•œ í”„ë¡œì íŠ¸ ìë™ ì±„ì  ê¸°ëŠ¥ ê°œë°œê¸° - ë„ì „ê³¼ í•´ê²° ê³¼ì •
categories:
  - Dev
tags:
  - gpt
  - automation
  - prompt
  - evaluation
last_modified_at: 2024-05-05T22:21:00-00:00
---

í•™ìŠµê´€ë¦¬í”Œë«í¼ ì—ì„œ ì¤‘ìš”í•œ ë¶€ë¶„ì¸ í”„ë¡œì íŠ¸ í‰ê°€ì˜ íë¦„ì— ëŒ€í•´ ì„¤ëª…í•´ë³´ë©´

í•™ìƒë“¤ì´ í”„ë¡œì íŠ¸ ê³¼ì œë¥¼ ìˆ˜í–‰í•˜ê³  ë‚˜ì„œ github URLë¡œ í”„ë¡œì íŠ¸ ê²°ê³¼ë¬¼ì„ ì œì¶œ â†’

í•´ë‹¹ ê³¼ì •ì— ì†í•´ìˆëŠ” ì½”ì¹˜ë“¤ì—ê²Œ í•´ë‹¹ ê³¼ì œì œì¶œê±´ì´ ëœë¤ìœ¼ë¡œ ë°°ì • â†’ 

ì½”ì¹˜ë“¤ì´ í”„ë¡œì íŠ¸ ê²°ê³¼ë¬¼ì„ í™•ì¸í•˜ê¸°ìœ„í•´ ì§ì ‘ github URL ì— ì ‘ì† â†’ 

github ìƒì˜ ì¥¬í”¼í„° ë…¸íŠ¸ë¶ ë‚´ìš©ì„ í™•ì¸ â†’

í”„ë¡œì íŠ¸ ë§ˆë‹¤ ì •í•´ì ¸ ìˆëŠ” í‰ê°€í•­ëª©ì„ í™•ì¸í•˜ì—¬ ì ìˆ˜ì™€ í”¼ë“œë°±ì„ ë‚¨ê¹€. 



ì¼ë ¨ì˜ ê³¼ì •ì—ì„œ GPTë¥¼ í™œìš©í•´ ìƒì‚°ì„±ì„ ë†’ì¼ìˆ˜ìˆì§€ì•Šì„ê¹Œ ê³ ë¯¼í•˜ì˜€ê³ 

í”„ë¡œì íŠ¸ github URLì— ì ‘ì†í–ˆì„ë•Œ í™•ì¸ê°€ëŠ¥í•œ code data & ê³¼ì •ë³„ í‰ê°€ê¸°ì¤€ ì„

í”„ë¡¬í”„íŠ¸ì— ì…ë ¥í•˜ë©´ í‰ê°€ê¸°ì¤€ë³„ ì ìˆ˜ë¥¼ ë§¤ê¸°ê³  ë˜ ì œì¶œí•œ í”„ë¡œì íŠ¸ì— ëŒ€í•´ ì½”ë©˜íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ìˆì§€ ì•Šì„ê¹Œ?  

ìƒê°í•´ë³´ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤.


ê°„ë‹¨íˆ poc ë¥¼ ì§„í–‰í•´ë³´ì•˜ì„ë–„ ìƒê°ë³´ë‹¤ ì„±ëŠ¥ì´ ë‚˜ì˜ì§€ ì•Šì•˜ê³  ê·¸ë ‡ê²Œ GPTë¥¼ ì´ìš©í•œ ìë™ì±„ì ! ê¸°ëŠ¥ ê°œë°œì„ ì‹œì‘í•˜ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤. 
ì´ì œë¶€í„° ì–´ë– í•œ ê³¼ì •ì„ ê±°ì³ í•´ë‹¹ ê¸°ëŠ¥ì„ ê°œë°œí•˜ì˜€ìœ¼ë©° ë˜ ì§„í–‰ì¤‘ ìƒê¸´ ì´ìŠˆë“¤ì„ ì–´ë–»ê²Œ í•´ê²°í–ˆëŠ”ì§€ì— ëŒ€í•´ ì„¤ëª…í•´ë³´ê² ìŠµë‹ˆë‹¤.

ë¨¼ì € ê¸°ì¡´ í•™ìŠµë°ì´í„°ë¥¼ ë¶„ì„í•˜ì˜€ì„ë•Œ í”„ë¡œì íŠ¸ëŠ” ìœ í˜•ì— ë”°ë¼

A(raw ì½”ë“œ), B(ê¸°íƒ€ ë¸”ë¡œê·¸ URL), C(github URL) ìœ¼ë¡œ ë‚˜ë‰˜ì—ˆê³ 

ê° ìœ í˜•ì˜ ë¹„ì¤‘ì´ 13%, 0.2%, 86% ë¡œ êµ¬ì„±ë˜ì–´ìˆìŒì„ í™•ì¸í•˜ì˜€ìŠµë‹ˆë‹¤.

ì–´ëŠì •ë„ í¬ë§·ì´ ì •í•´ì ¸ìˆëŠ” ìœ í˜•ì—ì„œ ìë™ì±„ì ì´ ê°€ëŠ¥í•˜ë‹¤ê³  íŒë‹¨ë˜ì—ˆê³ , í”„ë¡œì íŠ¸ ìœ í˜•ì´ Aì™€ Cì¸ ê²½ìš°ì— ìë™ì±„ì ì„ ì§„í–‰í•˜ëŠ” ê²ƒìœ¼ë¡œ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤.

A  ìœ í˜•ì˜ ê²½ìš° ì•„ë˜ì™€ ê°™ì€ json í˜•íƒœì˜ ë°ì´í„° ì…ë‹ˆë‹¤.

```json
[
	{
	"code": "import tensorflow as tf\r\nimport numpy as np\r\n\r\nprint(tf.__version__)\r\nprint(np.__version__)", 
	"submit_block_id": "f28f7c7d-f6a7-4f5c-9174-06d442be1455"
	}, 
	{
	"code": "import tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\nimport tensorflow_datasets as tfds", 
	"submit_block_id": "0d9ff2c5-0c80-4ae2-8cae-6d99f4789797"
	}, 
	{
	"code": "(ds_train, ds_test), ds_info = tfds.load(\r\n    'stanford_dogs',\r\n    split=['train', 'test'],\r\n    as_supervised=True,\r\n    shuffle_files=True,\r\n    with_info=True,\r\n)", 
	"submit_block_id": "0b2020d7-b79b-44fe-921c-0865726c34c1"
	}, 
	{
	"code": "def normalize_and_resize_img(image, label):\r\n    # Normalizes images: `uint8` -> `float32`\r\n    image = tf.image.resize(image, [224, 224])\r\n    return tf.cast(image, tf.float32) / 255., label\r\n\r\n# basic augmentation\r\ndef augment(image, label):\r\n    image = tf.image.random_flip_left_right(image)\r\n    image = tf.image.random_brightness(image, max_delta=0.2)\r\n    image = tf.clip_by_value(image, 0, 1)\r\n    return image, label\r\n\r\ndef onehot(image, label):\r\n    label = tf.one_hot(label, num_classes)\r\n    return image, label\r\n\r\ndef apply_normalize_on_dataset(ds, is_test=False, batch_size=16, with_aug=False, aug_method=None):\r\n    ds = ds.map(\r\n        normalize_and_resize_img, \r\n        num_parallel_calls=2\r\n    )\r\n    if not is_test and with_aug:\r\n        ds = ds.map(\r\n            augment\r\n        )\r\n    ds = ds.batch(batch_size)\r\n    # cutmix augmentation\r\n    if not is_test and aug_method=='cutmix':\r\n        ds = ds.map(\r\n            cutmix,\r\n            num_parallel_calls=2\r\n        )\r\n    # mixup augmentation\r\n    elif not is_test and aug_method=='mixup':\r\n        ds = ds.map(\r\n            mixup,\r\n            num_parallel_calls=2\r\n        )\r\n    else:\r\n        ds = ds.map(\r\n            onehot,\r\n            num_parallel_calls=2\r\n        )\r\n    if not is_test:\r\n        ds = ds.repeat()\r\n        ds = ds.shuffle(200)\r\n    ds = ds.prefetch(tf.data.experimental.AUTOTUNE)\r\n    return ds\r\n\r\nprint('=3')", "submit_block_id": "af271bfa-23ef-4ff6-813f-07942b4e10f4"}, {"code": "def get_clip_box(image_a, image_b):\r\n    # image.shape = (height, width, channel)\r\n    image_size_x = image_a.shape[1]\r\n    image_size_y = image_a.shape[0]\r\n    \r\n    # get center of box\r\n    x = tf.cast(tf.random.uniform([], 0, image_size_x), tf.int32)\r\n    y = tf.cast(tf.random.uniform([], 0, image_size_y), tf.int32)\r\n\r\n    # get width, height of box\r\n    width = tf.cast(image_size_x*tf.math.sqrt(1-tf.random.uniform([], 0, 1)), tf.int32)\r\n    height = tf.cast(image_size_y*tf.math.sqrt(1-tf.random.uniform([], 0, 1)), tf.int32)\r\n    \r\n    # clip box in image and get minmax bbox\r\n    x_min = tf.math.maximum(0, x-width//2)\r\n    y_min = tf.math.maximum(0, y-height//2)\r\n    x_max = tf.math.minimum(image_size_x, x+width//2)\r\n    y_max = tf.math.minimum(image_size_y, y+width//2)\r\n    \r\n    return x_min, y_min, x_max, y_max", "submit_block_id": "2aa0bfd5-9dff-45c2-a8c4-8d46acb6c5ef"}, {"code": "# mix two images\r\ndef mix_2_images(image_a, image_b, x_min, y_min, x_max, y_max):\r\n    image_size_x = image_a.shape[1]\r\n    image_size_y = image_a.shape[0] \r\n    middle_left = image_a[y_min:y_max, 0:x_min, :] # image_b\uc758 \uc67c\ucabd \ubc14\uae65 \uc601\uc5ed\r\n    middle_center = image_b[y_min:y_max, x_min:x_max, :]  # image_b\uc758 \uc548\ucabd \uc601\uc5ed\r\n    middle_right = image_a[y_min:y_max, x_max:image_size_x, :] # image_b\uc758 \uc624\ub978\ucabd \ubc14\uae65 \uc601\uc5ed\r\n    middle = tf.concat([middle_left,middle_center,middle_right], axis=1)\r\n    top = image_a[0:y_min, :, :]\r\n    bottom = image_a[y_max:image_size_y, :, :]\r\n    mixed_img = tf.concat([top, middle, bottom],axis=0)\r\n    \r\n    return mixed_img", "submit_block_id": "b3356272-2e8f-4a73-a8fc-9c5ff60694b2"}, {"code": "# mix two labels\r\ndef mix_2_labels(image_a, image_b, label_a, label_b, x_min, y_min, x_max, y_max, num_classes=120):\r\n    image_size_x = image_a.shape[1]\r\n    image_size_y = image_a.shape[0] \r\n    mixed_area = (x_max-x_min)*(y_max-y_min)\r\n    total_area = image_size_x*image_size_y\r\n    ratio = tf.cast(mixed_area/total_area, tf.float32)\r\n\r\n    if len(label_a.shape)==0:\r\n        label_a = tf.one_hot(label_a, num_classes)\r\n    if len(label_b.shape)==0:\r\n        label_b = tf.one_hot(label_b, num_classes)\r\n    mixed_label = (1-ratio)*label_a + ratio*label_b\r\n\r\n    return mixed_label", "submit_block_id": "9a8e3592-c60f-481b-b997-f64c4f6604ff"}, {"code": "def cutmix(image, label, prob=1.0, batch_size=16, img_size=224, num_classes=120):\r\n    mixed_imgs = []\r\n    mixed_labels = []\r\n\r\n    for i in range(batch_size):\r\n        image_a = image[i]\r\n        label_a = label[i]\r\n        j = tf.cast(tf.random.uniform([],0, batch_size),tf.int32)\r\n        image_b = image[j]\r\n        label_b = label[j]\r\n        x_min, y_min, x_max, y_max = get_clip_box(image_a, image_b)\r\n        mixed_imgs.append(mix_2_images(image_a, image_b, x_min, y_min, x_max, y_max))\r\n        mixed_labels.append(mix_2_labels(image_a, image_b, label_a, label_b, x_min, y_min, x_max, y_max))\r\n\r\n    mixed_imgs = tf.reshape(tf.stack(mixed_imgs), (batch_size, img_size, img_size, 3))\r\n    mixed_labels = tf.reshape(tf.stack(mixed_labels), (batch_size, num_classes))\r\n\r\n    return mixed_imgs, mixed_labels", "submit_block_id": "63323261-6aaf-4e51-b92d-173a245a0da0"}, {"code": "# function for mixup\r\ndef mixup_2_images(image_a, image_b, label_a, label_b):\r\n    ratio = tf.random.uniform([], 0, 1)\r\n    \r\n    if len(label_a.shape)==0:\r\n        label_a = tf.one_hot(label_a, num_classes)\r\n    if len(label_b.shape)==0:\r\n        label_b = tf.one_hot(label_b, num_classes)\r\n    mixed_image= (1-ratio)*image_a + ratio*image_b\r\n    mixed_label = (1-ratio)*label_a + ratio*label_b\r\n    \r\n    return mixed_image, mixed_label", "submit_block_id": "ae1b48eb-a750-4ac4-ac17-b011d9b5a5eb"}, {"code": "def mixup(image, label, prob=1.0, batch_size=16, img_size=224, num_classes=120):\r\n    mixed_imgs = []\r\n    mixed_labels = []\r\n\r\n    for i in range(batch_size):\r\n        image_a = image[i]\r\n        label_a = label[i]\r\n        j = tf.cast(tf.random.uniform([],0,batch_size), tf.int32)\r\n        image_b = image[j]\r\n        label_b = label[j]\r\n        mixed_img, mixed_label = mixup_2_images(image_a, image_b, label_a, label_b)\r\n        mixed_imgs.append(mixed_img)\r\n        mixed_labels.append(mixed_label)\r\n\r\n    mixed_imgs = tf.reshape(tf.stack(mixed_imgs), (batch_size, img_size, img_size, 3))\r\n    mixed_labels = tf.reshape(tf.stack(mixed_labels), (batch_size, num_classes))\r\n\r\n    return mixed_imgs, mixed_labels", "submit_block_id": "28c60d1c-c39c-40e8-93ac-b22e6139be64"}, {"code": "num_classes = ds_info.features[\"label\"].num_classes\r\n\r\nds_train_no_aug = apply_normalize_on_dataset(ds_train)\r\nds_train_aug = apply_normalize_on_dataset(ds_train, with_aug=True)\r\n\r\nds_train_cutmix = apply_normalize_on_dataset(ds_train, with_aug=True, aug_method='cutmix')\r\nds_train_mixup = apply_normalize_on_dataset(ds_train, with_aug=True, aug_method='mixup')\r\n\r\nds_test = apply_normalize_on_dataset(ds_test, is_test=True)", "submit_block_id": "518c373f-1f3b-49ec-85a0-584b670fee14"}, {"code": "#num_classes = ds_info.features[\"label\"].num_classes\r\nresnet50 = keras.models.Sequential([\r\n    keras.applications.resnet.ResNet50(\r\n        include_top=False,\r\n        weights='imagenet',\r\n        input_shape=(224,224,3),\r\n        pooling='avg',\r\n    ),\r\n    keras.layers.Dense(num_classes, activation='softmax')\r\n])", "submit_block_id": "305f8801-dd57-4b38-b989-ec0ee5953582"}, {"code": "aug_resnet50 = keras.models.Sequential([\r\n    keras.applications.resnet.ResNet50(\r\n        include_top=False,\r\n        weights='imagenet',\r\n        input_shape=(224, 224,3),\r\n        pooling='avg',\r\n    ),\r\n    keras.layers.Dense(num_classes, activation = 'softmax')\r\n])", "submit_block_id": "f67a154e-6501-4e8b-a26c-c4e78f25c7d1"}, {"code": "cutmix_resnet50 = keras.models.Sequential([\r\n    keras.applications.resnet.ResNet50(\r\n        include_top=False,\r\n        weights='imagenet',\r\n        input_shape=(224,224,3),\r\n        pooling='avg',\r\n    ),\r\n    keras.layers.Dense(num_classes, activation='softmax')\r\n])", "submit_block_id": "8f241cc2-8767-4d6a-b4a9-06e9f652a2a6"}, {"code": "mixup_resnet50 = keras.models.Sequential([\r\n    keras.applications.resnet.ResNet50(\r\n        include_top=False,\r\n        weights='imagenet',\r\n        input_shape=(224,224,3),\r\n        pooling='avg',\r\n    ),\r\n    keras.layers.Dense(num_classes, activation='softmax')\r\n])", "submit_block_id": "b247e983-cd02-45dd-9fa5-46e368a879d4"}, {"code": "# LMS \ud658\uacbd\uc744 \uae30\uc900\uc73c\ub85c, \ubaa8\ub378 \ud559\uc2b5\uc5d0 epoch\ub2f9 2~3\ubd84\uc758 \uc2dc\uac04\uc774 \uc18c\uc694\ub429\ub2c8\ub2e4.\r\n# \ud559\uc2b5\uc774 \ub108\ubb34 \uc624\ub798 \uac78\ub9b4 \uacbd\uc6b0 epoch \uc218\ub97c \uc870\uc808\ud574 \uc8fc\uc138\uc694\r\n\r\nEPOCH = 5", "submit_block_id": "50a52852-db35-4f22-b463-b67a8b0488d7"}, {"code": "# no augmentation\r\nresnet50.compile(\r\n    loss='categorical_crossentropy',\r\n    optimizer=tf.keras.optimizers.SGD(learning_rate=0.01),\r\n    metrics=['accuracy'],\r\n)\r\n\r\nhistory_no_aug = resnet50.fit(\r\n    ds_train_no_aug, # augmentation \uc801\uc6a9\ud558\uc9c0 \uc54a\uc740 \ub370\uc774\ud130\uc14b \uc0ac\uc6a9\r\n    steps_per_epoch=int(ds_info.splits['train'].num_examples/16),\r\n    validation_steps=int(ds_info.splits['test'].num_examples/16),\r\n    epochs=EPOCH,\r\n    validation_data=ds_test,\r\n    verbose=1,\r\n    use_multiprocessing=True,\r\n)", "submit_block_id": "fdc15c0b-e0f1-4c70-9031-829a6b335aba"}, {"code": "# basic augmentation\r\naug_resnet50.compile(\r\n    loss='categorical_crossentropy',\r\n    optimizer=tf.keras.optimizers.SGD(learning_rate=0.01),\r\n    metrics=['accuracy'],\r\n)\r\n\r\nhistory_aug = aug_resnet50.fit(\r\n    ds_train_aug, # augmentation \uc801\uc6a9\ud55c \ub370\uc774\ud130\uc14b \uc0ac\uc6a9\r\n    steps_per_epoch=int(ds_info.splits['train'].num_examples/16),\r\n    validation_steps=int(ds_info.splits['test'].num_examples/16),\r\n    epochs=EPOCH,\r\n    validation_data=ds_test,\r\n    verbose=1,\r\n    use_multiprocessing=True,\r\n)", "submit_block_id": "8c7c6714-fd9d-4e65-971e-4e1a983c1f85"}, {"code": "# basic augmentation + cutmix\r\ncutmix_resnet50.compile(\r\n    loss='categorical_crossentropy',\r\n    optimizer=tf.keras.optimizers.SGD(learning_rate=0.01),\r\n    metrics=['accuracy'],\r\n)\r\n\r\nhistory_cutmix = cutmix_resnet50.fit(\r\n    ds_train_cutmix, # cutmix \uc801\uc6a9\ud55c \ub370\uc774\ud130\uc14b \uc0ac\uc6a9\r\n    steps_per_epoch=int(ds_info.splits['train'].num_examples/16),\r\n    validation_steps=int(ds_info.splits['test'].num_examples/16),\r\n    epochs=EPOCH,\r\n    validation_data=ds_test,\r\n    verbose=1,\r\n    use_multiprocessing=True,\r\n)", "submit_block_id": "380aec46-b47b-4ec4-8516-b4f7890e5b31"}, {"code": "# basic augmentation + mixup\r\nmixup_resnet50.compile(\r\n    loss='categorical_crossentropy',\r\n    optimizer=tf.keras.optimizers.SGD(learning_rate=0.01),\r\n    metrics=['accuracy'],\r\n)\r\n\r\nhistory_mixup = mixup_resnet50.fit(\r\n    ds_train_mixup, # mixup \uc801\uc6a9\ud55c \ub370\uc774\ud130\uc14b \uc0ac\uc6a9\r\n    steps_per_epoch=int(ds_info.splits['train'].num_examples/16),\r\n    validation_steps=int(ds_info.splits['test'].num_examples/16),\r\n    epochs=EPOCH,\r\n    validation_data=ds_test,\r\n    verbose=1,\r\n    use_multiprocessing=True,\r\n)", "submit_block_id": "19e8bf71-d6a4-4fdd-a3c3-502912b6147d"}, {"code": "plt.figure(figsize=(12, 8))\r\nplt.plot(history_no_aug.history['val_accuracy'], 'r')\r\nplt.plot(history_aug.history['val_accuracy'], 'b')\r\nplt.plot(history_cutmix.history['val_accuracy'], 'g')\r\nplt.plot(history_mixup.history['val_accuracy'], 'k')\r\nplt.title('Model validation accuracy')\r\nplt.ylabel('Accuracy')\r\nplt.xlabel('Epoch')\r\nplt.legend(['no augmentation', 'with augmentation', 'with cutmix', 'with mixup'])\r\nplt.show()", "submit_block_id": "17794101-27ef-4fdd-accd-9099c71f59fd"
	}
]
```

C ìœ í˜•ì˜ ê²½ìš° ì•„ë˜ ì˜ˆì‹œì™€ ê°™ì€ jupyter notebook í˜•íƒœì…ë‹ˆë‹¤.

![jupyter notebook ì˜ˆì‹œ](/assets/images/jupyter_nodetbook_example.png)

í° êµ¬ì¡°ëŠ” í•™ìƒì´ í”„ë¡œì íŠ¸ë¥¼ ì œì¶œí• ë•Œì— ìœ„ í”„ë¡œì íŠ¸ ìœ í˜•(A, C)ì— í•´ë‹¹í•˜ëŠ”ì§€ ì²´í¬í•˜ì—¬

í•´ë‹¹ ì¡°ê±´ì„ ë§Œì¡±í•˜ë©´ gpt ìë™ì±„ì  taskë¥¼ celery ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹„ë™ê¸°ë¡œ í˜¸ì¶œí•˜ê³ 

ìë™ì±„ì  ê²°ê³¼ëŠ” ë³„ë„ DB(RubricGptEvaluation, GptEvaluation) ì— ì €ì¥í•˜ëŠ” êµ¬ì¡°ì…ë‹ˆë‹¤.

![ìë™ì±„ì  ìˆœì„œë„](/assets/images/gpt-auto-eval-diagram.png)

ì´í›„ ì½”ì¹˜ê°€ í”„ë¡œì íŠ¸ í‰ê°€í™”ë©´ì— ì ‘ì†í–ˆì„ë•Œ ìë™ì±„ì  ê²°ê³¼ê°€ ìˆë‹¤ë©´ í•´ë‹¹ ì •ë³´ë¥¼ í‘œê¸°í•˜ì—¬

í‰ê°€ë¥¼ í• ë•Œì— í•´ë‹¹ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ í‰ê°€ë¥¼ í• ìˆ˜ ìˆê²Œ í•˜ì˜€ìŠµë‹ˆë‹¤.

ë˜í•œ í•´ë‹¹ í™”ë©´ì—ëŠ” ì˜ëª» ì œì¶œëœ í”„ë¡œì íŠ¸ URLì„ ì½”ì¹˜ê°€ ì§ì ‘ ìˆ˜ì •í• ìˆ˜ ìˆê³ 
ë˜ ê°±ì‹ ëœ URL ê¸°ì¤€ìœ¼ë¡œ ìë™ì±„ì ì„ ë‹¤ì‹œ ìˆ˜í–‰ê°€ëŠ¥í•´ì•¼ í•œë‹¤ëŠ” ìš”êµ¬ì‚¬í•­ì„ ë°˜ì˜í•˜ì—¬ ì§ì ‘ gpt ìë™ì±„ì ì„ í˜¸ì¶œí•˜ì—¬ ê²°ê³¼ë¥¼ í™•ì¸í•  ìˆ˜ ìˆëŠ” ë²„íŠ¼ì„ ì¶”ê°€í•˜ì˜€ìŠµë‹ˆë‹¤.

```python
# task.py

def í”„ë¡œì íŠ¸_ì œì¶œí•¨ìˆ˜ ():
...

# GPT ìë™ ì±„ì  ì‹¤í–‰
if submit_type in ["projA", "projC"]:
    get_gpt_evaluation_feedback.delay(node_progrs_id, rubric_id_list, submit_type, submit_data)
		
...            
```

ë³„ë„ì˜ task.py íŒŒì¼ì„ ìƒì„±í•˜ì—¬ ì•„ë˜ì™€ ê°™ì´ êµ¬ì„±í•˜ì˜€ìŠµë‹ˆë‹¤.

get_gpt_evaluation_feedback() í•¨ìˆ˜ëŠ”

ì•ì„œ í”„ë¡œì íŠ¸_ì œì¶œí•¨ìˆ˜() ì—ì„œ í˜¸ì¶œí•˜ëŠ” task job ìœ¼ë¡œ

ìë™ì±„ì ì„ ìœ„í•œ í•™ìŠµì •ë³´ id, í‰ê°€ê¸°ì¤€ id, í”„ë¡œì íŠ¸ ìœ í˜•, í”„ë¡œì íŠ¸ data ë¥¼ ì¸ìë¡œ í•©ë‹ˆë‹¤.


```python
@shared_task
def get_gpt_evaluation_feedback(node_progrs_id, rubric_id_list, submit_type, submission_data):
    processed_data = process_submission(submit_type, submission_data) # 1
    if not processed_data:
        logger.warning(f"No processed data for NodeProgrs ID: {node_progrs_id}")
        return

    # Perform GPT evaluation
    try:
        rubric_info = get_rubric_details(rubric_id_list) # 2
        response_data = evaluate_with_gpt(node_progrs_id, rubric_info, processed_data) # 3
        save_gpt_evaluation(node_progrs_id, rubric_id_list, response_data) # 4
        logger.info(f"GPT evaluation feedback recorded for NodeProgrs ID: {node_progrs_id}")
    except Exception as e:
        logger.error(f"Error processing GPT evaluation feedback: {e}")
```

ìœ„ ì½”ë“œë¥¼ ìˆœì„œëŒ€ë¡œ ìì„¸íˆ ì‚´í´ë³´ë©´, #1 process_submission() ë©”ì„œë“œì—ì„œëŠ”

í”„ë¡œì íŠ¸ ìœ í˜•ì— ë”°ë¼ codeì™€ github URLì„ êµ¬ë¶„í•˜ì—¬ github URLì¸ ê²½ìš° íŒŒì¼ì˜ í™•ì¥ìê°€ ipynb ì¥¬í”¼í„° ë…¸íŠ¸ë¶ ì¸ì§€ ì²´í¬í•˜ê³  
"https://raw.githubusercontent.com/" ë¡œ ì‹œì‘í•˜ëŠ” github raw dataë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ìˆëŠ” ì£¼ì†Œë¡œ ë³€í™˜í•œ ì´í›„

parsing í•˜ì—¬ codeê°€ ë‹´ê¸´ code cell ë¶€ë¶„ë§Œ ê°€ì ¸ì˜¤ë„ë¡ ì²˜ë¦¬í•˜ì˜€ìŠµë‹ˆë‹¤.


#2 get_rubric_details() ë©”ì„œë“œì—ì„œëŠ”

í‰ê°€ê¸°ì¤€ idë¥¼ ê°€ì§€ê³  í‰ê°€ê¸°ì¤€ì— ëŒ€í•œ ìƒì„¸ì •ë³´ë¥¼ ì¡°íšŒí•˜ì—¬ ìë™ì±„ì ì„ ì§„í–‰ í•˜ê¸° ìœ„í•œ data ë¥¼ ì…‹ì—…í•˜ì˜€ìŠµë‹ˆë‹¤.

#3 evaluate_with_gpt() -call_chat_gpt_api()-  ë©”ì„œë“œì—ì„œëŠ”

ì£¼ì–´ì§„ ìë£Œë“¤ì„ ì¢…í•©í•˜ì—¬ gpt apië¥¼ í˜¸ì¶œ, ìë™ì±„ì  ì‘ë‹µì„ ì¶œë ¥í•©ë‹ˆë‹¤.

call_chat_gpt_api() ë©”ì„œë“œë¥¼ ë³´ë©´ (ì•„ë˜ì˜ ì½”ë“œë¥¼ ì°¸ê³ , evaluate_with_gpt() ë©”ì„œë“œ ë‚´ë¶€ë¥¼ í™•ì¸í•˜ë©´ call_chat_gpt_api()ë¥¼ í˜¸ì¶œí•˜ê²Œ ë˜ì–´ìˆê³  gpt api ë¥¼ í˜¸ì¶œí•˜ëŠ” ê´€ë ¨ë¡œì§ì€ call_chat_gpt_api() ë©”ì„œë“œì— ì‘ì„±ë˜ì–´ ìˆìŒ)

gptë¥¼ ì´ìš©í•´ ìë™ì±„ì ì„ í•˜ê¸° ìœ„í•´ì„  ì ì ˆí•œ í”„ë¡¬í”„íŠ¸ë¥¼ êµ¬ì„±í•´ì•¼í•©ë‹ˆë‹¤.

```
messages = [
        {
            "role": "system",
            "content": (
                "ë‹¹ì‹ ì€ AI ë©˜í† ë¡œì„œ í•™ìƒì˜ ì½”ë“œ ì œì¶œì„ í‰ê°€í•˜ê³  íŠ¹ì • JSON í˜•ì‹ìœ¼ë¡œ í‰ê°€ ê²°ê³¼ë¥¼ ì¶œë ¥í•˜ëŠ” ì„ë¬´ë¥¼ ë§¡ê³  ìˆìŠµë‹ˆë‹¤. "
                "ê° ë£¨ë¸Œë¦­ì— ëŒ€í•´ ì§€ì •ëœ ë²”ìœ„ ë‚´ì—ì„œ ì ìˆ˜ë¥¼ ë¶€ì—¬í•˜ê³  ìƒì„¸í•œ í”¼ë“œë°±ì„ ì œê³µí•´ì£¼ì„¸ìš”. "
                "ë§ˆì§€ë§‰ìœ¼ë¡œ ì „ì²´ í”„ë¡œì íŠ¸ì— ëŒ€í•œ ì¢…í•©ì ì¸ ì˜ê²¬ì„ í•œêµ­ì–´ë¡œ ì œê³µí•´ì£¼ì„¸ìš”. "
                "ê° ë£¨ë¸Œë¦­ ê¸°ì¤€ì— ëŒ€í•œ ìƒì„¸í•œ í”¼ë“œë°±ê³¼ ì „ì²´ ì˜ê²¬ì„ ë‹¨ë½ìœ¼ë¡œ êµ¬ì„±í•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œ ì œê³µí•´ì£¼ì„¸ìš”. ê²°ê³¼ë¬¼ì€ ëª…í™•í•œ ë‹¨ë½ êµ¬ì„±ê³¼ ë¬¸ì¥ ë³„ ê°œí–‰ì„ í¬í•¨í•˜ì—¬ ê°€ë…ì„±ì„ ë†’ì—¬ì£¼ì„¸ìš”:\n"
                "{\n"
                "    'rubric_evaluations': [\n"
                "        {'score': <ì ìˆ˜>, 'comment': <ì½”ë©˜íŠ¸>},\n"
                "        ... (ê° ë£¨ë¸Œë¦­ì— ëŒ€í•´)\n"
                "    ],\n"
                "    'overall_comment': <ì „ì²´ ì½”ë©˜íŠ¸>\n"
                "}"
            )
        },
        {
            "role": "system",
            "content": "Submitted Code for Evaluation:"
        },
        {
            "role": "system",
            "content": processed_data
        },
    ]
```

ì•„ë˜ì—ì„œ client.chat.completions.create() ì—ì„œ response_formatì„ json í˜•íƒœë¡œ ë”°ë¡œ ì§€ì •í•˜ê¸´ í–ˆì§€ë§Œ í”„ë¡¬í”„íŠ¸ ë‚´ì—ì„œë„ ì¶œë ¥ í¬ë§·ì„ ë‹¤ì‹œ í•œë²ˆ jsonìœ¼ë¡œ ëª…ì‹œì ìœ¼ë¡œ ì§€ì •í•˜ê³  ê° í‰ê°€ê¸°ì¤€(ë£¨ë¸Œë¦­)ì— ëŒ€í•´ì„œ ê°œë³„ ì ìˆ˜ì™€ ì˜ê²¬ì„ ì±„ì í•˜ê³ 

ë˜ í”„ë¡œì íŠ¸ ì „ì²´ì— ëŒ€í•œ ì˜ê²¬ì„ ì œê³µí•  ê²ƒì„ ì´ì•¼ê¸°í–ˆìŠµë‹ˆë‹¤.

í”„ë¡¬í”„íŠ¸ë¥¼ êµ¬ì„±í• ë•Œ ì‘ë‹µê°’ì˜ í˜•íƒœë¥¼ jsonìœ¼ë¡œ ë³´ì¥í•˜ê¸° ìœ„í•´ json modeê°€ ì§€ì›ë˜ëŠ” ëª¨ë¸ì„ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.

â€œgpt-4-1106-previewâ€ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì˜€ê³  response_formatì„ â€œjson_objectâ€ ë¡œ ì§€ì •í•˜ì˜€ìŠµë‹ˆë‹¤.

```python
response = client.chat.completions.create(
    model="gpt-4-1106-preview",
    response_format={"type": "json_object"},
    messages=messages
)
```

```
    rubric_descriptions = "ë‹¤ìŒì€ ì´ í”„ë¡œì íŠ¸ë¥¼ í‰ê°€í•˜ê¸° ìœ„í•œ ë£¨ë¸Œë¦­ ì„¸ë¶€ ì‚¬í•­ì…ë‹ˆë‹¤:\n\n"
    for rubric in rubric_info:
        rubric_descriptions += f"ë©”íŠ¸ë¦­ '{rubric['metric']}'ì€ 0ì  ë˜ëŠ” 1ì ìœ¼ë¡œ í‰ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìƒì„¸ í‰ê°€ ê¸°ì¤€ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n"
        for category in rubric['categories']:
            rubric_descriptions += f" - '{category['eval_standard']}'ì— ëŒ€í•œ ì ìˆ˜ {category['score']}\n"

    messages.append({
        "role": "user",
        "content": rubric_descriptions
    })

    messages.append({
        "role": "user",
        "content": (
            "ì´ì œ ì „ì²´ í”„ë¡œì íŠ¸ì— ëŒ€í•œ ì¢…í•©ì ì¸ ì˜ê²¬ì„ JSON í˜•ì‹ìœ¼ë¡œ ì œê³µí•´ì£¼ì„¸ìš”. ê²°ê³¼ë¬¼ì— ì ì ˆí•œ ì´ëª¨ì§€ë¥¼ í¬í•¨ì‹œì¼œ í•™ìŠµìì˜ ì´í•´ë¥¼ ë•ê³ , ê¸€ì˜ ì˜ë¯¸ë¥¼ ê°•ì¡°í•´ì£¼ì„¸ìš”.\n"
            "AI/SW ë©˜í† ë¡œì„œ í•™ìƒì˜ í”„ë¡œì íŠ¸ì— ëŒ€í•œ ê· í˜• ì¡íŒ, ëª…í™•í•˜ê³  êµ¬ì²´ì ì¸ í”¼ë“œë°±ì„ í•œêµ­ì–´ë¡œ ì œê³µí•´ì£¼ì„¸ìš”. í”„ë¡œì íŠ¸ì˜ ê°•ì ê³¼ ê°œì„ í•  ì ì„ ìƒì„¸íˆ ì–¸ê¸‰í•˜ê³ , í•™ìŠµìê°€ ë” ë°œì „í•  ìˆ˜ ìˆë„ë¡ ë„ì›€ì´ ë˜ëŠ” ì¶”ê°€ ìë£Œë‚˜ ì°¸ê³ í•  ë§Œí•œ ì‚¬ë¡€ë„ ì œì‹œí•´ì£¼ì„¸ìš”. \n"
            "ì¹œì ˆí•˜ê³  ê²©ë ¤í•˜ëŠ” ì–´ì¡°ë¡œ í”¼ë“œë°±ì„ êµ¬ì„±í•˜ì—¬ í•™ìŠµìì˜ ìì‹ ê°ì„ ì¦ì§„ì‹œí‚¤ê³ , ê¸°ìˆ ì  ì„±ì¥ì„ ìœ„í•œ ëª…í™•í•œ ë°©í–¥ì„ ì œì‹œí•´ì£¼ì„¸ìš”."
        )
    })
```

ê° í‰ê°€ê¸°ì¤€ì˜ ì ìˆ˜ë²”ìœ„ë¥¼ 0ì  ë˜ëŠ” 1ì ì´ë¼ê³  ëª…ì‹œí•˜ì˜€ìŠµë‹ˆë‹¤. ê·¸ë ‡ì§€ ì•Šê³  1ì ê¹Œì§€ ê°€ëŠ¥í•˜ë‹¤ê³  í•˜ì˜€ì„ë•ŒëŠ” ì†Œìˆ˜ì ì„ í¬í•¨í•œ ê°’ì„ ì¶œë ¥í•˜ê¸°ë„ í•˜ì—¬ 0ì  ë˜ëŠ” 1ì ì´ë¼ê³  ëª…ì‹œì ìœ¼ë¡œ ì´ì•¼ê¸°í•˜ì˜€ìŠµë‹ˆë‹¤.

í”„ë¡œì íŠ¸ ì „ì²´ì— ëŒ€í•œ í‰ê°€ì˜ê²¬ ë¶€ë¶„ì—ì„œëŠ” í•™ìƒì´ ëŠë¼ê¸°ì— ë”±ë”±í•˜ì§€ ì•Šê³  ì¹œê·¼í•˜ê²Œ ëŠë‚„ìˆ˜ìˆê²Œ ì–´ì¡°ì™€ ë‹´ê¸¸ ë‚´ìš©ì˜ ëª©í‘œë¥¼ ì§€ì •í•˜ì˜€ìŠµë‹ˆë‹¤.


ì¶”ê°€ë¡œ summary_data() ë¥¼ ì¡°íšŒí•˜ëŠ” ë¶€ë¶„ì´ ë‚˜ì˜¤ëŠ”ë° ì´ˆê¸° ê¸°ëŠ¥ ê°œë°œì‹œ ì„¤ê³„í–ˆë˜ ê²ƒì€ 

í‰ê°€ ë°ì´í„°ê°€ ì–´ëŠì •ë„ ìŒ“ì˜€ì„ë•Œ

ì ìˆ˜ë³„ ì´ì „ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ ìë™ì±„ì ì— ë°˜ì˜í•˜ë ¤ê³  í•˜ì˜€ìŠµë‹ˆë‹¤.

ì˜ˆë¥¼ ë“¤ë©´

0ì ì„ ë°›ì€ í‰ê°€ë°ì´í„°ì™€ ì½”ì¹˜ì˜ í”¼ë“œë°±â€¦ X nê°œ

1ì ì„ ë°›ì€ í‰ê°€ë°ì´í„°ì™€ ì½”ì¹˜ì˜ í”¼ë“œë°±â€¦ X nê°œ

2ì ì„ ë°›ì€ í‰ê°€ë°ì´í„°ì™€ ì½”ì¹˜ì˜ í”¼ë“œë°±â€¦ X nê°œ

3ì ì„ ë°›ì€ í‰ê°€ë°ì´í„°ì™€ ì½”ì¹˜ì˜ í”¼ë“œë°±â€¦ X nê°œ

+

í˜„ì¬ í•™ìƒì´ ì œì¶œí•œ í”„ë¡œì íŠ¸ ë°ì´í„° 

+

í‰ê°€ê¸°ì¤€

ê³¼ ê°™ì´ í”„ë¡¬í”„íŠ¸ë¥¼ êµ¬ì„±í•˜ì—¬ gptì˜ ì‘ë‹µì„ ì¢€ë” ê°•ê±´í•˜ê²Œ í•˜ê³ ì‹¶ì—ˆìŠµë‹ˆë‹¤. 
ì´ ë‚´ìš©ì€ ì´í›„ RAG ë¶€ë¶„ì—ì„œ ì–´ë–»ê²Œ ì§„í–‰í•´ ë³¼ ìˆ˜ ìˆì„ì§€ ë” ì´ì•¼ê¸°í•´ë³´ê² ìŠµë‹ˆë‹¤.

ì•„ë˜ëŠ” call_chat_gpt_api() í•¨ìˆ˜ì˜ ì „ì²´ ì½”ë“œì…ë‹ˆë‹¤.

```python
def call_chat_gpt_api(node_progrs_id, rubric_info, processed_data):
    logger.info(f"Calling Chat GPT API for NodeProgrs ID: {node_progrs_id}")
    summary_data = get_summary_evaluation_data(node_progrs_id)

    messages = [
        {
            "role": "system",
            "content": (
                "ë‹¹ì‹ ì€ AI ë©˜í† ë¡œì„œ í•™ìƒì˜ ì½”ë“œ ì œì¶œì„ í‰ê°€í•˜ê³  íŠ¹ì • JSON í˜•ì‹ìœ¼ë¡œ í‰ê°€ ê²°ê³¼ë¥¼ ì¶œë ¥í•˜ëŠ” ì„ë¬´ë¥¼ ë§¡ê³  ìˆìŠµë‹ˆë‹¤. "
                "ê° ë£¨ë¸Œë¦­ì— ëŒ€í•´ ì§€ì •ëœ ë²”ìœ„ ë‚´ì—ì„œ ì ìˆ˜ë¥¼ ë¶€ì—¬í•˜ê³  ìƒì„¸í•œ í”¼ë“œë°±ì„ ì œê³µí•´ì£¼ì„¸ìš”. "
                "ë§ˆì§€ë§‰ìœ¼ë¡œ ì „ì²´ í”„ë¡œì íŠ¸ì— ëŒ€í•œ ì¢…í•©ì ì¸ ì˜ê²¬ì„ í•œêµ­ì–´ë¡œ ì œê³µí•´ì£¼ì„¸ìš”. "
                "ê° ë£¨ë¸Œë¦­ ê¸°ì¤€ì— ëŒ€í•œ ìƒì„¸í•œ í”¼ë“œë°±ê³¼ ì „ì²´ ì˜ê²¬ì„ ë‹¨ë½ìœ¼ë¡œ êµ¬ì„±í•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œ ì œê³µí•´ì£¼ì„¸ìš”. ê²°ê³¼ë¬¼ì€ ëª…í™•í•œ ë‹¨ë½ êµ¬ì„±ê³¼ ë¬¸ì¥ ë³„ ê°œí–‰ì„ í¬í•¨í•˜ì—¬ ê°€ë…ì„±ì„ ë†’ì—¬ì£¼ì„¸ìš”:\n"
                "{\n"
                "    'rubric_evaluations': [\n"
                "        {'score': <ì ìˆ˜>, 'comment': <ì½”ë©˜íŠ¸>},\n"
                "        ... (ê° ë£¨ë¸Œë¦­ì— ëŒ€í•´)\n"
                "    ],\n"
                "    'overall_comment': <ì „ì²´ ì½”ë©˜íŠ¸>\n"
                "}"
            )
        },
        {
            "role": "system",
            "content": "Submitted Code for Evaluation:"
        },
        {
            "role": "system",
            "content": processed_data
        },
    ]

    if summary_data:
        messages.append({
            "role": "system",
            "content": "Summary of Previous Evaluations:"
        })

        messages.append({
            "role": "system",
            "content": summary_data  # ìš”ì•½ëœ í‰ê°€ ë‚´ìš©
        })

    rubric_descriptions = "ë‹¤ìŒì€ ì´ í”„ë¡œì íŠ¸ë¥¼ í‰ê°€í•˜ê¸° ìœ„í•œ ë£¨ë¸Œë¦­ ì„¸ë¶€ ì‚¬í•­ì…ë‹ˆë‹¤:\n\n"
    for rubric in rubric_info:
        rubric_descriptions += f"ë©”íŠ¸ë¦­ '{rubric['metric']}'ì€ 0ì  ë˜ëŠ” 1ì ìœ¼ë¡œ í‰ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìƒì„¸ í‰ê°€ ê¸°ì¤€ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n"
        for category in rubric['categories']:
            rubric_descriptions += f" - '{category['eval_standard']}'ì— ëŒ€í•œ ì ìˆ˜ {category['score']}\n"

    messages.append({
        "role": "user",
        "content": rubric_descriptions
    })

    messages.append({
        "role": "user",
        "content": (
            "ì´ì œ ì „ì²´ í”„ë¡œì íŠ¸ì— ëŒ€í•œ ì¢…í•©ì ì¸ ì˜ê²¬ì„ JSON í˜•ì‹ìœ¼ë¡œ ì œê³µí•´ì£¼ì„¸ìš”. ê²°ê³¼ë¬¼ì— ì ì ˆí•œ ì´ëª¨ì§€ë¥¼ í¬í•¨ì‹œì¼œ í•™ìŠµìì˜ ì´í•´ë¥¼ ë•ê³ , ê¸€ì˜ ì˜ë¯¸ë¥¼ ê°•ì¡°í•´ì£¼ì„¸ìš”.\n"
            "AI/SW ë©˜í† ë¡œì„œ í•™ìƒì˜ í”„ë¡œì íŠ¸ì— ëŒ€í•œ ê· í˜• ì¡íŒ, ëª…í™•í•˜ê³  êµ¬ì²´ì ì¸ í”¼ë“œë°±ì„ í•œêµ­ì–´ë¡œ ì œê³µí•´ì£¼ì„¸ìš”. í”„ë¡œì íŠ¸ì˜ ê°•ì ê³¼ ê°œì„ í•  ì ì„ ìƒì„¸íˆ ì–¸ê¸‰í•˜ê³ , í•™ìŠµìê°€ ë” ë°œì „í•  ìˆ˜ ìˆë„ë¡ ë„ì›€ì´ ë˜ëŠ” ì¶”ê°€ ìë£Œë‚˜ ì°¸ê³ í•  ë§Œí•œ ì‚¬ë¡€ë„ ì œì‹œí•´ì£¼ì„¸ìš”. \n"
            "ì¹œì ˆí•˜ê³  ê²©ë ¤í•˜ëŠ” ì–´ì¡°ë¡œ í”¼ë“œë°±ì„ êµ¬ì„±í•˜ì—¬ í•™ìŠµìì˜ ìì‹ ê°ì„ ì¦ì§„ì‹œí‚¤ê³ , ê¸°ìˆ ì  ì„±ì¥ì„ ìœ„í•œ ëª…í™•í•œ ë°©í–¥ì„ ì œì‹œí•´ì£¼ì„¸ìš”."
        )
    })

    try:
        response = client.chat.completions.create(
            model="gpt-4-1106-preview",
            response_format={"type": "json_object"},
            messages=messages
        )
        logger.info(f"Received response from Chat GPT API for NodeProgrs ID: {node_progrs_id}")
        print(response.choices[0].message.content)
        return response.choices[0].message.content
    except Exception as e:
        logger.error(f"Error calling Chat GPT API: {e}")
        return None
```

ì´ì–´ì§€ëŠ” #4 save_gpt_evaluation() ë©”ì„œë“œì—ì„œëŠ”

ìë™ì±„ì ëœ ê²°ê³¼ë¥¼ ë³„ë„ ìƒì„±í•œ í…Œì´ë¸” RubricGptEvaluation, GptEvaluation ì— ì €ì¥í•©ë‹ˆë‹¤.

RubricGptEvaluation ì—ëŠ” í‰ê°€ idë³„  í‰ê°€ê¸°ì¤€ ì ìˆ˜ì™€ í‰ê°€ì˜ê²¬ì„ ì €ì¥í•˜ì˜€ê³ 

GptEvaluation ì—ëŠ” í•™ìŠµì •ë³´ id ë³„ ì „ì²´ í‰ê°€ì˜ê²¬ì„ ì €ì¥í•˜ì˜€ìŠµë‹ˆë‹¤. 

ì½”ì¹˜ê°€ í”„ë¡œì íŠ¸ í‰ê°€í™”ë©´ì— ì ‘ì†í•˜ì˜€ì„ë•Œ ìë™ì±„ì ëœ ê²°ê³¼ê°€ ìˆëŠ” ê²½ìš°ì— ìë™ì±„ì  ê²°ê³¼ë¥¼ í™•ì¸í•  ìˆ˜ ìˆê²Œ í•˜ì˜€ìŠµë‹ˆë‹¤.

ë˜í•œ ìë™ì±„ì  ëŒ€ìƒì´ì§€ë§Œ ìë™ì±„ì ì‹œ ì—ëŸ¬ê°€ ë°œìƒí•˜ì—¬ ìë™ì±„ì  ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš° 

ë˜ëŠ” ì‚¬ìš©ìì˜ ì˜¤ì…ë ¥ìœ¼ë¡œ ë‹¤ì‹œ ì±„ì í•´ì•¼í•˜ëŠ” ê²½ìš°ì™€ ê°™ì€ ì˜ˆì™¸ ê²½ìš°ë¥¼ ê³ ë ¤í•˜ì—¬

ìë™ì±„ì ì„ ì§ì ‘ ì§„í–‰í•˜ê¸° ìœ„í•´ call_chat_gpt_api()ë¥¼ í˜¸ì¶œí•˜ëŠ” apië¥¼ ë”°ë¡œ êµ¬í˜„í•˜ì˜€ìŠµë‹ˆë‹¤.

![ìë™ì±„ì  ê²°ê³¼ê°€ í™•ì¸ê°€ëŠ¥í•œ í”„ë¡œì íŠ¸ í‰ê°€í™”ë©´ ì˜ˆì‹œ](/assets/images/auto_evaluation_screen.png)

ì½”ì¹˜ê°€ í‰ê°€í™”ë©´ì— ì ‘ì†í•˜ë©´ (ìœ„ ì˜ˆì‹œí™”ë©´) ì™¼ìª½ì—ëŠ” í‰ê°€ ê¸°ì¤€ë³„ gpt ìë™ì±„ì  ê²°ê³¼,  

í”„ë¡œì íŠ¸ ì „ì²´ì— ëŒ€í•œ í‰ê°€ì˜ê²¬ ìˆœì„œë¡œ í™•ì¸í• ìˆ˜ ìˆìœ¼ë©° 

í•´ë‹¹ ë°ì´í„°ë“¤ì€ ì‹¤ì œ í‰ê°€ì— í™œìš©ë  ìˆ˜ ìˆì„ë¿ ì‹¤ì œ í‰ê°€ê²°ê³¼ì— ë°”ë¡œ ì ìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

ìë™ì±„ì  ê²°ê³¼ë¥¼ í™•ì¸í•˜ê³  ì˜ëª»ëœ ë¶€ë¶„ì´ ìˆë‹¤ë©´ ìˆ˜ì •í›„ í‰ê°€ë¥¼ ì§„í–‰í•  í•„ìš”ê°€ ìˆë‹¤ê³  ìƒê°ë˜ì—ˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.

ì½”ì¹˜ëŠ” í•´ë‹¹ ìë™ì±„ì  ê²°ê³¼ ë°ì´í„°ë¥¼ í™œìš©í•  ìˆ˜ ìˆê³  ë³¸ì¸ì´ ì§ì ‘ í•™ìƒì˜ í”„ë¡œì íŠ¸ í‰ê°€ì˜ê²¬ì„ ë‚¨ê¸¸ìˆ˜ìˆìŠµë‹ˆë‹¤.

ë˜í•œ ìë™ì±„ì  ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš° Github URLì„ ì…ë ¥í•˜ê³  ìš°ì¸¡ ìƒë‹¨ì˜ â€œGPT ì±„ì í•˜ê¸°â€ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ìë™ì±„ì ì„ ì§ì ‘ ìš”ì²­í• ìˆ˜ ìˆìŠµë‹ˆë‹¤.



ìƒ˜í”Œë§í•œ gpt ìë™ì±„ì  ê²°ê³¼ë¥¼ í™•ì¸í•´ë³´ë©´ í‰ê°€ê¸°ì¤€ ë³„ ìŠ¤ì½”ì–´ì™€ í‰ê°€ì˜ê²¬ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤. 

í•´ë‹¹í•˜ëŠ” í‰ê°€ì ìˆ˜ì™€ ì´ëª¨í‹°ì½˜ì´ í¬í•¨ëœ í‰ê°€ì˜ê²¬ì„ í™•ì¸í• ìˆ˜ ìˆìŠµë‹ˆë‹¤.

0 ì /	í‰ê°€ ì§€í‘œ ì„ íƒ ë° ê·¼ê±° ì„œìˆ : í”„ë¡œì íŠ¸ì—ì„œëŠ” ë‹¤ì–‘í•œ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ ì‚¬ìš©í•˜ê³ , ê° ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ê¸° ìœ„í•´ **'classification_report'**ë¥¼ í™œìš©í•˜ì…¨ìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ê° ëª¨ë¸ì— ì í•©í•œ í‰ê°€ ì§€í‘œ ì„ íƒ ì´ìœ ì™€ ê·¼ê±°ì— ëŒ€í•œ ì„œìˆ ì´ ëˆ„ë½ë˜ì–´ ìˆì–´, ì´ ë¶€ë¶„ì€ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤. ğŸš§

1 ì /	ëª¨ë¸ í•™ìŠµ ë° í‰ê°€: ë‹¤ì–‘í•œ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸(Decision Tree, **Random** Forest, SVC, SGD Classifier, Logistic Regression, XGBClassifier)ì„ ì ìš©í•˜ì—¬ í•™ìŠµ ë° í…ŒìŠ¤íŠ¸ë¥¼ ìˆ˜í–‰í•˜ê³ , ê° ëª¨ë¸ì— ëŒ€í•œ ì„±ëŠ¥ í‰ê°€ë¥¼ **'classification_report'**ë¥¼ í†µí•´ ì •ìƒì ìœ¼ë¡œ ì¶œë ¥í•œ ì ì´ ì¸ìƒì ì…ë‹ˆë‹¤. ëª¨ë“  ëª¨ë¸ì´ ì •ìƒ ì‘ë™í•˜ì—¬ ê²°ê³¼ê°’ì„ ì–»ì–´ë‚´ì…¨ê¸° ë•Œë¬¸ì— ì´ ë¶€ë¶„ì— ëŒ€í•´ ë§Œì ì„ ë¶€ì—¬í•©ë‹ˆë‹¤. âœ…

1 ì /	ë°ì´í„° ì…‹ êµ¬ì„± íŒŒì•… ë° ë°ì´í„° ì´í•´: í•™ìŠµìê»˜ì„œ ì œì¶œí•˜ì‹  ì½”ë“œë¥¼ ì‚´í´ë³´ì•˜ì„ ë•Œ, â€˜digitsâ€™ ë°ì´í„°ì…‹ì— ëŒ€í•œ feature ë° labelì˜ ì„ ì •ì„ ìœ„í•´ ê¸°ë³¸ì ì¸ ë°ì´í„° ë¶„ì„ì„ ìˆ˜í–‰í•˜ì‹  ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ, `**data**.head()`, `target`, `target_names`, `value_counts()` ë° `**describe**()` ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ì…‹ì˜ êµ¬ì„±ì„ íŒŒì•…í•¨ìœ¼ë¡œì¨ ë°ì´í„°ë¥¼ ì¶©ë¶„íˆ ì´í•´í•˜ë ¤ëŠ” ë…¸ë ¥ì´ ì—¿ë³´ì…ë‹ˆë‹¤. ğŸ“Š



ì•„ë˜ëŠ” í”„ë¡œì íŠ¸ ì „ì²´ì— ëŒ€í•œ gpt ìë™ì±„ì  í‰ê°€ì˜ê²¬ì…ë‹ˆë‹¤.


ì•ˆë…•í•˜ì„¸ìš”! í”„ë¡œì íŠ¸ë¥¼ ì—´ì‹¬íˆ ìˆ˜í–‰í•´ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤.

ìš°ì„ , ë°ì´í„° ë¶„ì„ë¶€í„° ë‹¤ì–‘í•œ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ ì ìš©í•˜ì—¬ ì‹¤í—˜í•œ ì ì€ ë§¤ìš° í›Œë¥­í•©ë‹ˆë‹¤.

ğŸ‰ íŠ¹íˆ, í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì„ ìœ„í•´ RandomizedSearchCVë¥¼ ì‚¬ìš©í•œ ë¶€ë¶„ì€ í•™ìŠµì˜ íƒ„íƒ„í•œ ì´í•´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œ ì¢‹ì€ ì ‘ê·¼ì…ë‹ˆë‹¤.

ğŸ‘ í•˜ì§€ë§Œ, í‰ê°€ ì§€í‘œ ì„ íƒê³¼ ê·¸ì— ëŒ€í•œ ê·¼ê±°ê°€ ëª…í™•í•˜ê²Œ ì„œìˆ ë˜ì§€ ì•Šì•˜ì–´ìš”.

ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•  ë•Œ ê° ì§€í‘œê°€ ê°€ì§€ëŠ” ì˜ë¯¸ì™€ ì¤‘ìš”ì„±ì„ ì´í•´í•˜ê³ , í”„ë¡œì íŠ¸ì˜ ëª©ì ê³¼ ë°ì´í„°ì˜ íŠ¹ì„±ì— ë”°ë¼ ì í•©í•œ ì§€í‘œë¥¼ ì„ íƒí•˜ëŠ” ê²ƒì€ ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤.

ì˜ˆë¥¼ ë“¤ì–´, í´ë˜ìŠ¤ ë¶ˆê· í˜•ì´ ìˆëŠ” ê²½ìš° ì •í™•ë„(Accuracy)ë§Œìœ¼ë¡œëŠ” ì¶©ë¶„í•œ í‰ê°€ê°€ ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, F1 ìŠ¤ì½”ì–´ë‚˜ AUC-ROC ë“±ì˜ ì§€í‘œë¥¼ í•¨ê»˜ ê³ ë ¤í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ë˜í•œ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë”ìš± í–¥ìƒì‹œí‚¤ê³ ì í•œë‹¤ë©´ êµì°¨ ê²€ì¦ ë°©ë²•, ì•™ìƒë¸” ëª¨ë¸ì˜ ì‚¬ìš© ë“± ë‹¤ì–‘í•œ ì‹œë„ë¥¼ í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ë‹¤ìŒ í”„ë¡œì íŠ¸ì—ëŠ” ê° ì§€í‘œë“¤ì´ ì™œ ì¤‘ìš”í•œì§€ë¥¼ ëª…ì‹œí•˜ë©´ì„œ, ìì‹ ì˜ ì„ íƒì— ëŒ€í•œ íƒ€ë‹¹í•œ ê·¼ê±°ë¥¼ ì¶”ê°€í•´ë³´ì‹œê¸¸ ë°”ëë‹ˆë‹¤.

í˜ë‚´ì„¸ìš”! ğŸ’ª



ìë™ì±„ì  ê¸°ëŠ¥ ê°œë°œì€ ë¬´ì‚¬íˆ ì™„ë£Œë˜ì—ˆì§€ë§Œ

ì‹¤ì œ ìš´ì˜í™˜ê²½ì— ë°˜ì˜í•˜ê³  ë‚˜ì„œ í™•ì¸í•œ ì´ìŠˆë“¤ì´ ìˆì—ˆìŠµë‹ˆë‹¤.

- context ê¸¸ì´ ì´ˆê³¼ë¡œ ì—ëŸ¬ ë°œìƒ

ë¨¼ì € ì•„ì£¼ ë“œë¬¼ê²Œ ë°œìƒí•˜ê¸´ í–ˆì§€ë§Œ ì œì¶œí•œ github url ìƒì˜ code ë°ì´í„°ê°€ ê¸¸ì–´ 

gpt apië¥¼ í˜¸ì¶œì‹œ context ê¸¸ì´ ì´ˆê³¼ë¡œ ì—ëŸ¬ê°€ ë°œìƒí•˜ê³  ì´í›„ì— ìë™ì±„ì ì´ ì§„í–‰ë˜ì§€ ì•ŠëŠ” ê²½ìš°ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.

ì˜ˆë¥¼ ë“¤ë©´ ì•„ë˜ì™€ ê°™ì€ ì—ëŸ¬ê°€ ë°œìƒí•˜ì˜€ëŠ”ë°,

`Error calling Chat GPT API: Error code: 400 - {'error': {'message': "This model's maximum context length is 16385 tokens. However, your messages resulted in 23305 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}`

í•´ë‹¹ ë©”ì‹œì§€ëŠ” í”„ë¡¬í”„íŠ¸ ê¸¸ì´ê°€ ì •í•´ì§„ `16385` tokenì„ ì´ˆê³¼í•˜ì—¬ `23305` tokenì´ë¼ ë©”ì‹œì§€ ê¸¸ì´ë¥¼ ì¤„ì—¬ì•¼ í•œë‹¤ëŠ” ë‚´ìš©ì…ë‹ˆë‹¤.

ì¼ë°˜ì ìœ¼ë¡œ ìƒê°í–ˆì„ë•Œ ë©”ì‹œì§€ ê¸¸ì´ë¥¼ ë‹¨ìˆœíˆ ì¤„ì´ëŠ” ê²ƒì„ ìƒê°í•  ìˆ˜ ìˆì§€ë§Œ 

ì´ë ‡ê²Œ ë˜ë©´ í‰ê°€ì— ì¤‘ìš”í•œ ì •ë³´ê°€ ë’¤ì— ìˆëŠ” ê²½ìš° ì „ì²´ ìë™ì±„ì ì— ì˜í–¥ì„ ë¯¸ì¹ ìˆ˜ ìˆì–´ì„œ ì–´ë–»ê²Œ ì¤„ì¼ì§€ì— ëŒ€í•´ ê³ ë¯¼ì„ í•´ì•¼í–ˆìŠµë‹ˆë‹¤.

ë˜ ë‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œëŠ” tokenì˜ ìµœëŒ€ê¸¸ì´ë¥¼ ì´ˆê³¼í•˜ëŠ” ê²½ìš° ë©”ì‹œì§€ë¥¼ ë¶„í• í•´ì„œ ì „ì†¡í•˜ëŠ” ë°©ë²•,

ì˜ˆë¥¼ ë“¤ì–´ 16385 tokenì„ ì´ˆê³¼í•˜ëŠ” ê²½ìš° ë©”ì‹œì§€ë¥¼ 16385 token ë‹¨ìœ„ë¡œ ë¶„í• í•œ ë‹¤ìŒ 

ê°ê°ì˜ ê²°ê³¼ë¥¼ í™•ì¸í•´ ì¢…í•©í•˜ëŠ” ë°©ë²•ì´ ìˆìŠµë‹ˆë‹¤.


2ê°€ì§€ ë°©ë²•ì¤‘ ì¢€ ë” ì í•©í•œ ê²ƒì€ ë¶„í• í•´ì„œ ì „ì†¡í•œ í›„ ê°ê°ì˜ ê²°ê³¼ê°’ì„ ë‹¤ì‹œ ì¢…í•©í•˜ëŠ” 2ë²ˆ ë°©ë²•ì´ì—ˆìœ¼ë‚˜

ë§ˆì§€ë§‰ì— ì‹¤ì œ ì ìš©í•œ ë°©ë²•ì€ ë‹¤ë¥¸ ëª¨ë¸ì„ ì‚¬ìš© í•˜ëŠ” ê²ƒì´ì—ˆìŠµë‹ˆë‹¤.


json ëª¨ë“œë¥¼ ì‚¬ìš©í•´ì•¼í•˜ëŠ” ì¡°ê±´ì´ ìˆì—ˆê¸°ì— í•´ë‹¹ ê¸°ëŠ¥ì„ ì œê³µí•˜ëŠ” ëª¨ë¸ì¤‘ì— ì„ íƒì„ í–ˆìœ¼ë©°

ìµœì´ˆ ëª¨ë¸ì€  â€œgpt-3.5-turbo-1106â€ ë¥¼ ì‚¬ìš©í•˜ì˜€ê³  

CONTEXT WINDOWê°€ 16,385 tokensì´ê³  training data ì‹œì ì´ 2021ë…„ 9ì›”ì´ì—ˆìŠµë‹ˆë‹¤.

ì´í›„ â€œgpt-4-1106-previewâ€ ëª¨ë¸ì˜ ê²½ìš°

CONTEXT WINDOWê°€ 128,000 tokensì´ê³  training data ì‹œì ë„ 2023ë…„ 4ì›”ë¡œ í™•ì—°í•œ ì°¨ì´ë¥¼ ë³´ì˜€ë‹¤.

ëª¨ë¸ ì—…ë°ì´íŠ¸ ì‹œì  ì´í›„ì—ëŠ” í•´ë‹¹ ì—ëŸ¬ëŠ” ë”°ë¡œ ë°œìƒí•˜ì§€ì•Šì•˜ìŠµë‹ˆë‹¤.



- RAG (Retrieval-Augmented Generation)

ì•ì—ì„œë„ ì ê¹ ì–¸ê¸‰í•˜ì˜€ì§€ë§Œ í‰ê°€ ë°ì´í„°ê°€ ì–´ëŠì •ë„ ìŒ“ì˜€ë‹¤ë©´ ìƒˆë¡œìš´ í”„ë¡œì íŠ¸ ê±´ì— ëŒ€í•´ ìë™ì±„ì ì‹œ

ì´ë¯¸ í‰ê°€ë˜ì–´ ì°¸ê³ í•  ë°ì´í„°ê°€ ìˆëŠ”ì§€ ì¡°íšŒí•˜ëŠ” ë¶€ë¶„ì´ ìˆì—ˆìŠµë‹ˆë‹¤.

ì´ë¯¸ ìŒ“ì—¬ìˆëŠ” ìë™ì±„ì  ë°ì´í„°ë¥¼ ì‹ ê·œ ì±„ì ê±´ì— ë°˜ì˜í•˜ëŠ” ê²ƒì€ 

í‰ê°€ ì¼ê´€ì„±ì„ ìœ ì§€í•˜ê³  í‰ê°€ í’ˆì§ˆì˜ í–¥ìƒì„ ê¸°ëŒ€í• ìˆ˜ìˆìœ¼ë©° í‰ê°€ ì‹œê°„ì„ ë‹¨ì¶•í•  ìˆ˜ ìˆë‹¤ëŠ” ì´ì ì´ ìˆìŠµë‹ˆë‹¤.



í–¥í›„ RAGì„ êµ¬ì„±í•˜ê²Œ ëœë‹¤ë©´ ì•„ë˜ì™€ ê°™ì€ ë°©ë²•ìœ¼ë¡œ ì§„í–‰í•´ë³¼ ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤.

-.ìˆ˜ì§‘ëœ ê³¼ê±° ìë™ì±„ì  ë°ì´í„°ë¥¼ í•„ìš”í•œ ê²½ìš° ì •ì œí•˜ê±°ë‚˜ ì „ì²˜ë¦¬

-.ì§ˆì˜ ì‘ë‹µ ìŒì„ ìƒì„±. ì˜ˆë¥¼ ë“¤ì–´ ê³¼ì œ ë‚´ìš©ê³¼ ì½”ë“œë¥¼ ì§ˆì˜ë¡œ ê·¸ì— ëŒ€í•œ ì±„ì  ê²°ê³¼ì™€ í”¼ë“œë°±ì„ ì‘ë‹µìœ¼ë¡œ

-.ì§ˆì˜ ì‘ë‹µ ìŒì„ íš¨ê³¼ì ìœ¼ë¡œ ê²€ìƒ‰í• ìˆ˜ ìˆë„ë¡ ElasticSearch, Lucene ê³¼ ê°™ì€ ê²€ìƒ‰ì—”ì§„ì„ í™œìš©í•˜ì—¬ ìƒ‰ì¸êµ¬ì¶•

-.ìƒˆë¡œìš´ ê³¼ì œê°€ ì œì¶œë˜ë©´ ê³¼ì œì˜ ë‚´ìš©ê³¼ ì½”ë“œë¥¼ ì§ˆì˜ë¡œ ì‚¬ìš©í•˜ì—¬ ê¸°ì¡´ ì±„ì  ë°ì´í„°ì—ì„œ ê°€ì¥ ìœ ì‚¬í•œ ê³¼ì œë¥¼ ê²€ìƒ‰

-.ê²€ìƒ‰ëœ ìœ ì‚¬ ê³¼ì œì˜ ì±„ì ê²°ê³¼ì™€ í”¼ë“œë°±ì„ í˜„ì¬ ê³¼ì œì˜ ìë™ì±„ì ì— í™œìš©

![ê·¸ë¦¼ 2.LLMê³¼ í•¨ê»˜ RAGë¥¼ ì‚¬ìš©í•˜ëŠ” ê°œë…ì  íë¦„ -ì¶œì²˜: https://aws.amazon.com/ko/what-is/retrieval-augmented-generation/](/assets/images/jumpstart-fm-rag.jpg)
ê·¸ë¦¼ 2.LLMê³¼ í•¨ê»˜ RAGë¥¼ ì‚¬ìš©í•˜ëŠ” ê°œë…ì  íë¦„ -ì¶œì²˜: https://aws.amazon.com/ko/what-is/retrieval-augmented-generation/



- ìë™ì±„ì ì„ ì‚¬ìš©ìê°€ ì§ì ‘ ì‹¤í–‰ì‹œ ê²°ê³¼ê°’ì„ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì „ë‹¬í• ìˆ˜ ìˆì„ê¹Œ?

```python
from rest_framework.response import StreamingHttpResponse
from django.http import HttpResponse

class ProcessGptEvaluationView(APIView):
    permission_classes = [CoachOnly]

    def post(self, request, node_progrs_id):
        submission_id = request.data.get('submission_id')
        submit_type = request.data.get('submit_type')
        submit_data = request.data.get('submit_data')

        # ... (ê¸°ì¡´ ì½”ë“œ ìƒëµ) ...

        # Perform GPT evaluation
        try:
            rubric_info = get_rubric_details(rubric_id_list)
            
            # ChatGPT API í˜¸ì¶œ ì‹œ stream=True ì„¤ì •
            response = openai.ChatCompletion.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "You are an AI assistant."},
                    {"role": "user", "content": processed_data}
                ],
                stream=True
            )

            def generate_response():
                collected_chunks = []
                for chunk in response:
                    chunk_message = chunk['choices'][0]['delta'].get('content', '')
                    collected_chunks.append(chunk_message)
                    yield chunk_message

                # ì „ì²´ ì‘ë‹µ ë©”ì‹œì§€ ì¡°í•©
                full_response = ''.join(collected_chunks)

                # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
                save_gpt_evaluation(node_progrs_id, rubric_id_list, full_response)
                logger.info(f"GPT evaluation feedback recorded for NodeProgrs ID: {node_progrs_id}")

            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ë°˜í™˜
            return StreamingHttpResponse(generate_response(), content_type='text/plain')

        except Exception as e:
            logger.error(f"Error in GPT evaluation: {str(e)}")
            return HttpResponse(status=500)
```

ìœ„ ProcessGptEvaluationView () ì½”ë“œëŠ” ì§ì ‘ gpt apië¥¼ í˜¸ì¶œí•˜ì—¬ ìë™ì±„ì ì„ ì§„í–‰í•˜ëŠ” ê¸°ì¡´ ì½”ë“œë¥¼

stream í˜•íƒœë¡œ ë³€í™˜í•œ ì½”ë“œì…ë‹ˆë‹¤.

ê²°ê³¼ê°’ì„ stream í™” í•˜ê¸° ìœ„í•´ì„ 

ë¨¼ì € openai.ChatCompletion.create() ì—ì„œ stream íŒŒë¼ë¯¸í„°ì˜ ê°’ì„ Trueë¡œ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.

ê·¸ë¦¬ê³  `generate_response()` ì œë„ˆë ˆì´í„° í•¨ìˆ˜ë¥¼ ì •ì˜í•˜ì—¬  ì‘ë‹µì„ chunk ë‹¨ìœ„ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.

collected_chunks ë¦¬ìŠ¤íŠ¸ì— chunk ë¥¼ ë³„ë„ ì €ì¥í•©ë‹ˆë‹¤.

ì „ì²´ ì‘ë‹µì´ ìˆ˜ì‹ ì™„ë£Œë˜ë©´ ê¸°ì¡´ ì‘ë‹µ ë©”ì‹œì§€ë¥¼ full_response ë¡œ ì¡°í•©í•˜ê³  save_gpt_evaluation() ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•©ë‹ˆë‹¤.

ìœ„ ì½”ë“œì˜ ë§ˆì§€ë§‰ ë¶€ë¶„ì„ ë³´ë©´ StreamingHttpResponse ë¥¼ ì‚¬ìš©í•˜ì—¬ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ë°˜í™˜í•©ë‹ˆë‹¤.

return StreamingHttpResponse(generate_response(), content_type='text/plain')

StreamingHttpResponse ì—ëŠ” ì•ì„œ ì„ ì–¸í•œ generate_response() ì œë„ˆë ˆì´í„°ë¥¼ ì‘ë‹µì˜ ì½˜í…ì¸ ë¡œ ì „ë‹¬í•©ë‹ˆë‹¤.

ìœ„ì™€ ê°™ì´ ì½”ë“œë¥¼ ì‘ì„±í•˜ë©´ gpt í™”ë©´ì—ì„œì²˜ëŸ¼ ì‘ë‹µê°’ì„ ì¼ì •ë‹¨ìœ„ë¡œ ì „ë‹¬ë°›ì•„ í™”ë©´ì—ì„œ í™•ì¸í• ìˆ˜ ìˆìŠµë‹ˆë‹¤.